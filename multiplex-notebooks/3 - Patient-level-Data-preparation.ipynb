{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient-Level Merging\n",
    "\n",
    "Inputs\n",
    "Clinical Table\n",
    "\n",
    "- File: Clini_Table_Vorversuche_HER2.xlsx\n",
    "- Column: Patho-ID\n",
    "- HDF5 Feature Directory (result directory from previous Kronos embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "patients_file = Path(\n",
    "    \"/mnt/bulk-sirius/nguyenmin/multiplex/Clini_Table_Vorversuche_HER2.xlsx\"\n",
    ")  # <-- YOUR .xls or .csv\n",
    "h5_input_dir = Path(\n",
    "    \"/mnt/bulk-neptune/laura/multiplex/features\"\n",
    ")  # <-- raw h5 directory\n",
    "h5_output_dir = Path(\n",
    "    \"/mnt/bulk-sirius/nguyenmin/multiplex/temp_results\"\n",
    ")  # <-- save here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded patient table: (81, 5)\n",
      "Unique patients: 80\n",
      "Patients with H5 files found: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging patients:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing patient E-19-10396, 72 files\n",
      "→ Output: /mnt/bulk-sirius/nguyenmin/multiplex/temp_results/E-19-10396_component_data.h5\n",
      "Saved merged H5 with 1080 patches\n",
      "\n",
      "\n",
      "Processing patient E-19-14186, 148 files\n",
      "→ Output: /mnt/bulk-sirius/nguyenmin/multiplex/temp_results/E-19-14186_component_data.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging patients:  25%|██▌       | 2/8 [00:00<00:00,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged H5 with 2220 patches\n",
      "\n",
      "\n",
      "Processing patient E-19-1584, 164 files\n",
      "→ Output: /mnt/bulk-sirius/nguyenmin/multiplex/temp_results/E-19-1584_component_data.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging patients:  38%|███▊      | 3/8 [00:01<00:03,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged H5 with 2460 patches\n",
      "\n",
      "\n",
      "Processing patient E-19-17360, 130 files\n",
      "→ Output: /mnt/bulk-sirius/nguyenmin/multiplex/temp_results/E-19-17360_component_data.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging patients:  38%|███▊      | 3/8 [00:03<00:05,  1.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 109\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProcessing patient \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(flist)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m files\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m→ Output:\u001b[39m\u001b[33m\"\u001b[39m, outpath)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m merged = \u001b[43mmerge_patch_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATASET_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m h5py.File(outpath, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    112\u001b[39m     f.create_dataset(DATASET_NAME, data=merged, dtype=\u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mmerge_patch_embeddings\u001b[39m\u001b[34m(h5_list, dataset_name)\u001b[39m\n\u001b[32m     80\u001b[39m arrays = []\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m h5_list:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m dataset_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/bulk-neptune/laura/multiplex/STAMP/.venv/lib/python3.12/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n\u001b[32m    560\u001b[39m                      **kwds)\n\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/bulk-neptune/laura/multiplex/STAMP/.venv/lib/python3.12/site-packages/h5py/_hl/files.py:238\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[32m    237\u001b[39m         flags |= h5f.ACC_SWMR_READ\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    240\u001b[39m     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:102\u001b[39m, in \u001b[36mh5py.h5f.open\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5g.pyx:281\u001b[39m, in \u001b[36mh5py.h5g.GroupID.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5g.pyx:282\u001b[39m, in \u001b[36mh5py.h5g.GroupID.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:645\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "h5_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# OPTIONAL: Name of the column that contains Patho-ID\n",
    "PATIENT_COL = \"Patho-ID\"\n",
    "\n",
    "# Name of dataset inside h5 to merge\n",
    "DATASET_NAME = \"patch_embeddings\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LOAD PATIENT LIST\n",
    "# ------------------------------------------------------------\n",
    "if patients_file.suffix.lower() in [\".xls\", \".xlsx\"]:\n",
    "    df = pd.read_excel(patients_file)\n",
    "else:\n",
    "    df = pd.read_csv(patients_file)\n",
    "\n",
    "print(\"Loaded patient table:\", df.shape)\n",
    "\n",
    "# Extract patient prefixes like \"E/19/10396\"\n",
    "raw_ids = df[PATIENT_COL].dropna().astype(str).tolist()\n",
    "\n",
    "\n",
    "# Convert \"E/19/10396-2B\" → \"E-19-10396\"\n",
    "def normalize_patient_id(pid):\n",
    "    \"\"\"\n",
    "    Remove cell suffix (-2B), change / to -, keep only 'E-19-10396'\n",
    "    \"\"\"\n",
    "    pid = pid.strip()\n",
    "    pid = pid.split(\"-\")[0]  # remove trailing -2B\n",
    "    pid = pid.replace(\"/\", \"-\")  # E/19/10396 → E-19-10396\n",
    "    return pid\n",
    "\n",
    "\n",
    "patient_list = [normalize_patient_id(pid) for pid in raw_ids]\n",
    "patient_set = set(patient_list)\n",
    "\n",
    "print(\"Unique patients:\", len(patient_set))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. FIND ALL H5 FILES THAT BELONG TO ANY PATIENT\n",
    "# ------------------------------------------------------------\n",
    "files = sorted(h5_input_dir.glob(\"*.h5\"))\n",
    "\n",
    "# Map patient → list of h5 files\n",
    "patient_to_files = defaultdict(list)\n",
    "\n",
    "\n",
    "def extract_patient_from_h5name(fname):\n",
    "    \"\"\"\n",
    "    For E-19-10396_[10856,55448]_component_data_Her2.h5\n",
    "    return 'E-19-10396'\n",
    "    \"\"\"\n",
    "    base = fname.split(\"_\")[0]  # E-19-10396\n",
    "    return base\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    pid = extract_patient_from_h5name(f.stem)\n",
    "    if pid in patient_set:\n",
    "        patient_to_files[pid].append(f)\n",
    "\n",
    "print(\"Patients with H5 files found:\", len(patient_to_files))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. MERGE ALL H5 FILES PER PATIENT\n",
    "# ------------------------------------------------------------\n",
    "def merge_patch_embeddings(h5_list, dataset_name=DATASET_NAME):\n",
    "    \"\"\"\n",
    "    Load `patch_embeddings` from all h5s and concatenate.\n",
    "    \"\"\"\n",
    "    arrays = []\n",
    "    for path in h5_list:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            if dataset_name not in f:\n",
    "                raise ValueError(f\"{dataset_name} not found in {path}\")\n",
    "            arrays.append(f[dataset_name][:])\n",
    "    return np.concatenate(arrays, axis=0)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. SAVE MERGED FILES\n",
    "# ------------------------------------------------------------\n",
    "for patient, flist in tqdm(patient_to_files.items(), desc=\"Merging patients\"):\n",
    "    # Example flist contains many files like:\n",
    "    # E-19-10396_[10856,55448]_component_data_Her2.h5\n",
    "\n",
    "    # Detect marker name from the FIRST file\n",
    "    # e.g., \"component_data_Her2\"\n",
    "    first_name = flist[0].stem\n",
    "    marker_part = \"_\".join(first_name.split(\"_\")[2:])  # component_data_Her2\n",
    "\n",
    "    # Build output filename:\n",
    "    # E-19-10396_component_data_Her2.h5\n",
    "    outname = f\"{patient}_{marker_part}.h5\"\n",
    "    outpath = h5_output_dir / outname\n",
    "\n",
    "    print(f\"\\nProcessing patient {patient}, {len(flist)} files\")\n",
    "    print(\"→ Output:\", outpath)\n",
    "\n",
    "    merged = merge_patch_embeddings(flist, DATASET_NAME)\n",
    "\n",
    "    with h5py.File(outpath, \"w\") as f:\n",
    "        f.create_dataset(DATASET_NAME, data=merged, dtype=\"f\")\n",
    "\n",
    "    print(f\"Saved merged H5 with {merged.shape[0]} patches\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slide–to–Clinical Table Mapping and CSV Generation\n",
    "\n",
    "This notebook step constructs a slide-level mapping between multiplex feature files (.h5) and the corresponding clinical metadata rows in clinical table. The objective is to produce a clean slides.csv file that links each processed slide to its correct Patho-ID entry for downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "h5_dir = Path(\"/mnt/bulk-sirius/nguyenmin/multiplex/features/her2_feats_patient_level\")\n",
    "clini_table = Path(\n",
    "    \"/mnt/bulk-sirius/nguyenmin/multiplex/Clini_Table_Vorversuche_HER2.xlsx\"\n",
    ")\n",
    "output_csv = Path(\"/mnt/bulk-sirius/nguyenmin/multiplex/slides.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD CLINICAL TABLE\n",
    "# -----------------------------\n",
    "df_clini = pd.read_excel(clini_table)\n",
    "\n",
    "# Example column: \"Patho-ID\" contains strings like \"E/19/10396-2B\"\n",
    "raw_ids = df_clini[\"Patho-ID\"].dropna().astype(str).tolist()\n",
    "\n",
    "# Build mapping:\n",
    "#   E-19-10396 --> E/19/10396-2B\n",
    "#   E-19-1584  --> E/19/1584-2D\n",
    "patient_map = {}\n",
    "\n",
    "for pid in raw_ids:\n",
    "    trimmed = pid.split(\"-\")[0].replace(\"/\", \"-\")  # E/19/10396-2B → E-19-10396\n",
    "    patient_map[trimmed] = pid  # map trimmed → full\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: extract trimmed ID from filename\n",
    "# -----------------------------\n",
    "def extract_trimmed_id(fname: str) -> str:\n",
    "    # \"E-19-10396_[coords]_component...\" → \"E-19-10396\"\n",
    "    return fname.split(\"_\")[0]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Scan h5 files and build CSV entries\n",
    "# -----------------------------\n",
    "entries = []\n",
    "\n",
    "for h5_path in sorted(h5_dir.glob(\"*.h5\")):\n",
    "    fname = h5_path.name\n",
    "    trimmed = extract_trimmed_id(h5_path.stem)  # E-19-10396\n",
    "\n",
    "    if trimmed not in patient_map:\n",
    "        print(f\"WARNING: No clinical Patho-ID found for {trimmed}\")\n",
    "        continue\n",
    "\n",
    "    full_pid = patient_map[trimmed]  # E/19/10396-2B\n",
    "\n",
    "    entries.append(\n",
    "        {\n",
    "            \"Patho-ID\": full_pid,\n",
    "            \"FILENAME\": fname,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Save to CSV\n",
    "# -----------------------------\n",
    "df = pd.DataFrame(entries)\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} slides to {output_csv}\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
